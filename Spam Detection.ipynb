{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5857fcd2",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration/Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/Users/mac/Desktop/Spam_Detection/spam.csv\", encoding=\"latin-1\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afa097",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())  # View the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())  # Get information about data types and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc239ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b94b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of colors (red for spam and green for ham)\n",
    "colors = [\"red\", \"green\"]\n",
    "# Import libraries (if not already imported)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define label mapping \n",
    "label_map = {\"spam\": 0, \"ham\": 1}\n",
    "\n",
    "# Separate message lengths based on labels\n",
    "spam_lengths = []\n",
    "ham_lengths = []\n",
    "for message, label in zip(data['v2'], data['v1']):\n",
    "  length = len(message)\n",
    "  if label_map[label] == 0:  # Spam message\n",
    "    spam_lengths.append(length)\n",
    "  else:  # Ham message\n",
    "    ham_lengths.append(length)\n",
    "\n",
    "# Create histograms \n",
    "plt.hist([spam_lengths, ham_lengths], bins=20, stacked=True, label=['Spam', 'Ham'], color=colors)\n",
    "#plt.hist(spam_lengths, bins=10, alpha=1, label='Spam', color=colors[0])  # Color-code spam\n",
    "#plt.hist(ham_lengths, bins=15, alpha=0.5, label='Ham', color=colors[1])  # Color-code ham\n",
    "plt.xlabel('Message Length')\n",
    "plt.ylabel('Number of Messages')\n",
    "plt.title('Message Length Distribution (Spam vs. Ham)')\n",
    "plt.legend()  # Add legend to show color coding\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa05781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define stop words \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = [word for word in text.split() if word not in stop_words]  # Remove stop words\n",
    "    return words  # Return list of words (no joining)\n",
    "\n",
    "# Separate messages based on label ('spam' and 'ham' labels)\n",
    "spam_messages = []\n",
    "ham_messages = []\n",
    "for message, label in zip(data['v2'], data['v1']):\n",
    "  if label == \"spam\":\n",
    "    spam_messages.append(preprocess_text(message))\n",
    "  else:\n",
    "    ham_messages.append(preprocess_text(message))\n",
    "\n",
    "# Get most frequent words (without repetition)\n",
    "def get_top_words(messages, n_words):\n",
    "  all_words = sum(messages, [])  # Flatten the list of lists\n",
    "  word_counts = Counter(all_words)  # Count word frequencies\n",
    "  return [word for word, count in word_counts.most_common(n_words)]  # Get top n words\n",
    "\n",
    "# Get top 10 most frequent words in spam and ham\n",
    "spam_top_words = get_top_words(spam_messages, 10)\n",
    "ham_top_words = get_top_words(ham_messages, 10)\n",
    "\n",
    "# Print the top words\n",
    "print(\"Top 10 Words in Spam Messages:\")\n",
    "print(*spam_top_words, sep=\"\\n\")  # Print each word on a new line\n",
    "\n",
    "print(\"\\nTop 10 Words in Ham Messages:\")\n",
    "print(*ham_top_words, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d14820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define stop words \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = [word for word in text.split() if word not in stop_words]  # Remove stop words\n",
    "    return \" \".join(words)  # Join words back into a string\n",
    "\n",
    "# Separate messages based on label ('spam' and 'ham' labels)\n",
    "spam_messages = []\n",
    "ham_messages = []\n",
    "for message, label in zip(data['v2'], data['v1']):\n",
    "  if label == \"spam\":\n",
    "    spam_messages.append(preprocess_text(message))\n",
    "  else:\n",
    "    ham_messages.append(preprocess_text(message))\n",
    "\n",
    "# Create word cloud for spam messages\n",
    "spam_text = \" \".join(spam_messages)\n",
    "spam_wordcloud = WordCloud(width=400, height=300).generate(spam_text)\n",
    "\n",
    "# Create word cloud for ham messages\n",
    "ham_text = \" \".join(ham_messages)\n",
    "ham_wordcloud = WordCloud(width=400, height=300).generate(ham_text)\n",
    "\n",
    "# Plot the word clouds side-by-side\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(spam_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Spam Messages\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(ham_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Ham Messages\")\n",
    "\n",
    "plt.suptitle(\"Word Clouds for Spam vs. Ham Messages\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfad07",
   "metadata": {},
   "source": [
    "# Feature Engineeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_valid_url(text):\n",
    "  url_regex = r\"(?i)\\b((https?://|ftp://|www\\.)\\S+[^\\s.,>)\\]])\"  # Case-insensitive regex\n",
    "  return bool(re.findall(url_regex, text))\n",
    "\n",
    "data['has_url'] = data['v2'].apply(has_valid_url)  # Apply function to 'v2' column (message)\n",
    "\n",
    "# Convert True/False values to 1/0 \n",
    "data['has_url'] = data['has_url'].astype(int)  # Convert boolean to integer\n",
    "\n",
    "print(data.head(5)) # Print the first few rows to see the new column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c88b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['has_url']==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0e95f",
   "metadata": {},
   "source": [
    "# Choosing the model/Training/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def drop_columns_by_index(df, indexes_to_drop):\n",
    "  \"\"\"\n",
    "  Drops columns from a DataFrame based on a list of indexes.\n",
    "\n",
    "  Args:\n",
    "      df (pandas.DataFrame): The DataFrame to modify.\n",
    "      indexes_to_drop (list): A list of integer indexes of columns to drop.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The modified DataFrame with the specified columns dropped.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if indexes_to_drop is a list\n",
    "  if not isinstance(indexes_to_drop, list):\n",
    "    raise ValueError(\"indexes_to_drop must be a list of integers\")\n",
    "\n",
    "  # Validate indexes are within range\n",
    "  if any(i < 0 or i >= len(df.columns) for i in indexes_to_drop):\n",
    "    raise ValueError(\"Indexes to drop are out of range\")\n",
    "\n",
    "  # Drop columns using list comprehension (modifies df in-place)\n",
    "  df.drop(columns=[df.columns[i] for i in indexes_to_drop], inplace=True)\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "indexes_to_drop = [2, 3, 4]  # Drop columns at index 2, 3 and 4 (Unnamed)\n",
    "\n",
    "try:\n",
    "  df = drop_columns_by_index(df, indexes_to_drop)\n",
    "  print(df.head())\n",
    "except ValueError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77186bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = data['v2']  # Select the message column containing text data\n",
    "y = data['v1']  # Select the label column containing spam/ham labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "print(type(X_train))  # Should be list\n",
    "print(type(X_train[0]))  # Should be str (string)\n",
    "print(X_train[:2])  # Check the content of the first two elements\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for basic text preprocessing \n",
    "def preprocess_text(text):\n",
    "  \"\"\"\n",
    "  This function performs basic text preprocessing steps.\n",
    "  You can customize it further based on your data and needs.\n",
    "  \"\"\"\n",
    "  text = text.lower()  # Lowercase conversion\n",
    "  tokens = text.split()  # Basic tokenization\n",
    "\n",
    "  # Remove punctuation and non-alphanumeric characters \n",
    "  import string\n",
    "  punctuation = set(string.punctuation)\n",
    "  tokens = [word.lower() for word in tokens if word not in punctuation]\n",
    "\n",
    "  # Join tokens back into a single preprocessed string\n",
    "  preprocessed_text = \" \".join(tokens)  # Combine tokens with spaces\n",
    "\n",
    "  return preprocessed_text\n",
    "\n",
    "\n",
    "# Preprocess training and testing data\n",
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e51615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train_preprocessed))\n",
    "print(X_train_preprocessed[:2])  # Print the first two elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  \n",
    "X_train_features = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_features = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels (y_train and y_test are numerical)\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Train Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "# Train the model using encoded labels\n",
    "model.fit(X_train_features, y_train_encoded)\n",
    "\n",
    "# Make predictions using encoded test features\n",
    "y_pred = model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation using encoded labels\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred)\n",
    "recall = recall_score(y_test_encoded, y_pred)\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095aef4",
   "metadata": {},
   "source": [
    "# Thanks for watching!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
